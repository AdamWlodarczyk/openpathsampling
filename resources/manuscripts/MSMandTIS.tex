%% LyX 2.0.7.1 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english,draft,aps,onecolumn,showkeys,nofootinbib,superscriptaddress,showpacs,floatfix]{tufte-handout}
\usepackage{mathpazo}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{amsmath}

\makeatletter

\title{Adaptive Markov State Model Generation using Multi-State Transition
Interface Sampling}
\date{Draft}
\pdfpageheight\paperheight
\pdfpagewidth\paperwidth


\usepackage{mystyle}

\makeatother

\usepackage{babel}
\begin{document}
\maketitle
\marginnote[-3.7em]{Amsterdam, July 2014}
\begin{abstract}
  \marginnote[-2.4em]{Jan-Hendrik Prinz} \marginnote[-1.1em]{MSKCC,
    New York}
  \marginnote[0.2em]{\href{mailto:jan.prinz@choderalab.org}{jan.prinz@choderalab.org}}

  The application of small-sized Markov state models (MSMs) has become
  a standard tool for the analysis of the dynamics in molecular
  dynamics (MD) simulations and computation of key properties. Here we
  present a rigorous approach to enhance the generation of a MSM for a
  system of biomolecules using multi-state transition interface
  sampling (MSTIS)
\end{abstract}
\vspace{0.2cm}



\section{Authors}


\subsection{Jan-Hendrik Prinz }
\begin{quote}
  Memorial Sloan-Kettering Cancer Center, Zuckerman Research
  Institute, Zuckerman Research Center, 417 East 68th Street, New
  York, NY 10065, US

  \href{mailto:jan.prinz@choderalab.org}{jan.prinz@choderalab.org}
\end{quote}

\subsection{Weina Du}
\begin{quote}
  University of Amsterdam, The Netherland

  weina.du@...
\end{quote}

\subsection{Peter Bolhuis}
\begin{quote}
  bolhuis@...

  University of Amsterdam, The Netherland
\end{quote}

\subsection{John D. Chodera}
\begin{quote}
  Memorial Sloan-Kettering Cancer Center, Zuckerman Research
  Institute, Zuckerman Research Center, 417 East 68th Street, New
  York, NY 10065, US

  \href{mailto:john.chodera@choderalab.org}{john.chodera@choderalab.org}
\end{quote}

\section{Keywords / PACS}
\begin{quote}
  Time Series Analysis, Timescale Estimation, Markov State Models,
  Implied Timescales
\end{quote}

\section{Introduction}

\footnote{MSM Introduction from PMM Theory Draft%
}In recent years Markov State Models (MSMs) have matured into a useful
tool for the descriptions of dynamics that shows stochastic, but
metastable behavior on the timescales of interest
\citep{Prinz:2011id}. This includes protein dynamics, chemical
{[}cite{]}, physical systems {[}cite{]}, etc. Its application spans
dynamics that can be written using a differential equation that is
first order in time and has also been applied for various types of
dynamics, e.g. Brownian dynamics, Langevin dynamics, etc... Once a MSM
has been succesfully parametrized from data it allows to quickly
compute a large variety of target properties like relaxation
timescales, mean first passage times, metastable subsets
\citep{Roblitz:2013cg,DeuflhardWeber_PCCA} and path related properties
like reactive pathways and committor probabilities
\citep{VandenEijnden:2006ki,Metzner:2009ub}. The usage of conditional
jump probabilities also allows to effectively split the generation of
necessary statistics into independent subproblems that require only
local equilibrium and can be thus be parallelized as used in adaptive
sampling techniques.

Transition Interface Sampling from


\section{General Considerations}

TIS uses interfaces

We want to find core sets and regions around it. For a single state it
would be best to use the mean first passage time to


\subsection{Interface definition}

We want to define interfaces of a more complex shape than just based
on a metric based distance to a cluster center in some space. The
Voronoi cell discretization provides a natural way to do this. We just
need to define a cutoff for the Voronoi cells to ensure that the
interface is bounded. For the innermost interface we do exactly
this. Find generators for a Voronoi tesselation with a cut-off $\rho$
and then decide which cell should belong to the inner region of which
the boundary defines the interface. After this decision we need to
keep all points that are within a $2\rho$ radius since only these
points can contribute to the shape of the interface.

For the next one we do exactly the same thing but allow only
generators outside the core region. And then we cluster all points
that are outside the previously generated interface using these
generators with a new cut-off. And so on for the next interfaces. This
will produce a nested set of interfaces.


\subsection{Voronoi Problems}

We need to be aware that we 'cannot'%
\footnote{'cannot' means that this operation is roughly of the order
  $\mathcal{O}(N^{\text{dim}/2})$ and thus \emph{very} expensive in high
  dimensions.%
} compute the size of each Voronoi cell. At least we do not want to do
so and this means that the stationary distribution $\pi$ contains the
unknown factor of the phase space volume of its associated cell.
Although the stationary distribution is correct in the sense that it
discribes the probability for the process to be in the cell the value
can e.g. be large either because the probability density is high or
the cell is large and is thus problematic as a measure of importance
in terms of a macro state representative or cluster center.

Alternatively we will present a way to use average time to reach a
state when started from equilibrium or maybe the minimal roundtrip
time as a replacement. This is based on the fact that the most stable
center to the state with the highest potential energy has the tendency
that it is easiest to reach or return to that state compared to other
ones {[}maybe cite Bovier (metastability){]}. Eventually we can get a
way to estimate the size of the cell from the dynamics.

This problem also means that the left eigenvectors computed suffer
from the same problem. Especially if in a dense region we find many
cluster centers the probability density might be misleading and so the
left eigenvectors


\section{Distance Ideas from Transition matrices}


\subsection{Commute Distance}

The average roundtrip time between states $i$ and $j$

\begin{equation}
  \text{cd}\left[i\leftrightarrow j\right]=\tr{(\v{e}_{i}-\v{e}_{j})}\left(\mPi\left(\Id-\m{T}\right)\right)^{\text{-}}(\v{e}_{i}-\v{e}_{j})
\end{equation}



\subsection{Mean First Passage Time}

Definition of $\m{A}$ and $\m{W}$

\begin{equation}
  \m{A}\equiv\Id-\m{T}
\end{equation}


\begin{eqnarray*}
  \m{W} & \equiv & \Id-\m{A}\m{A}^{\#}\\
  & = & \m{J}\mPi
\end{eqnarray*}
Generlized inverse (Use the Jordan Decomposition and use the pseudo
inverse only on the diagonal part

\begin{eqnarray*}
  \m{A}^{\#} & \equiv & \text{JE}[\m{A}]\text{JD}\left[\m{A}\right]^{\dagger}\inv{\text{JE}[\m{A}]}\\
  & = & \inv{\left(\m{A}+\m{W}\right)}-\m{W}\\
  & = & \inv{\left(\Id-\m{T}+\m{J}\mPi\right)}-\m{J}\mPi
\end{eqnarray*}
The matrix $\m{A}^{\#}$ can be interpreted as a list of vectors that
contain the expected difference in counts between timeseries started
from equilibrium and timeseries started in each state.
\begin{equation}
  \m{A}_{j}^{\#}=\sum_{k=0}^{\infty}\left(\tr{\v{e}}_{j}\m{T}^{k}-\tr{\vpi}\right)
\end{equation}
The indexing with \emph{dg, }like\emph{ }$\m{A}_{dg}$, means to take
matrix $\m{A}$ and set all off-diagonal entries to zero. Mean First
Passage Time

\begin{eqnarray*}
  \m{M}=\text{mfpt}\left[i\rightarrow j\right] & = & \m{A}_{\v{x}\mid x_{0}=i}\left[\min_{n\ge1}x_{n}=j\right]\\
  & = & \left(\Id-\m{A}^{\#}+\m{J}\m{A}_{dg}^{\#}\right)\inv{\mPi}\\
  & = & \left(\Id-\inv{\left(\m{A}+\m{W}\right)}+\m{W}+\m{J}\m{A}_{dg}^{\#}\right)\inv{\mPi}\\
  & = & \left(\Id-\inv{\left(\m{A}+\m{J}\mPi\right)}+\m{J}\mPi+\m{J}\m{A}_{dg}^{\#}\right)\inv{\mPi}
\end{eqnarray*}


This can be used to compute the expected time to reach a state when
started in equilibrium (first discovery time)

\begin{eqnarray*}
  \text{fdt}[k] & = & \m{A}_{\v{x}\mid x_{0}\sim\pi}\left[\min_{n}x_{n}=k\right]\\
  & = & \left(\tr{\vpi}\m{M}\right)_{k}\\
  & = & 1+\frac{\m{A}_{kk}^{\#}}{\pi_{k}}
\end{eqnarray*}


We can also compute the variance in the mean first passage time by

\begin{eqnarray*}
  \m{V}_{ij} & = & \text{vmfpt}[i\rightarrow j]=\m{A}_{\v{x}\mid x_{0}=i}\left[\left(\min_{n\ge1}x_{n}=j-\m{M}_{ij}\right)^{2}\right]\\
  \m{V}_{ij} & = & \m{B}_{ij}-\left(\m{M}_{ij}\right)^{2}
\end{eqnarray*}
with the squared fluctuations given by

\begin{equation}
  \m{B}=\m{M}\left(2\m{A}_{dg}^{\#}\m{D}+\Id\right)+2\left(\m{A}^{\#}\m{M}-\m{J}(\m{A}^{\#}\m{M})_{dg}\right)
\end{equation}
which can be simplified for the diagonal elements
\begin{equation}
  \m{B}_{dg}=2\m{D}\m{A}_{dg}^{\#}\m{D}+\m{D}
\end{equation}
I assume we could use this to check how fast a certain region might be
explored up to a certain percentage.


\subsection{Multi-state Committor}

Probability to enter a state $i$ before visiting any other state
$j\neq i$.

Using OOM we can also change the expression for the committor to
trajectories of a maximal length which is very useful. See Multi-State
MFPT for the extension to the distribution for a trajectory length
$k$.

This directly gives also the probability to not hit any core!

Assume that we have
\begin{equation}
  \m{T}_{V}=\diag\left(\{\begin{cases}
      1 & \text{if }i\in\m{V}\\
      0 & \text{else}
    \end{cases}\mid i\in\Omega\}\right)\,\m{T}
\end{equation}
which is the the transition matrix with all columns set to zero that
belong to states that are not in $\m{V}$ the set of states not in a
core, also referred to as the \emph{void}.

We then split the rank deficient matrix into eigenvalues and left and
right eigenvectors like

\begin{equation}
  \m{T}_{V}=\m{R}_{V}\diag(\lambda_{V})\m{L}_{V}
\end{equation}
then the max length committor is given by
\begin{equation}
  C_{k}=\m{R}_{V}\diag(\frac{\lambda_{V}^{len}-1}{\lambda_{V}-1})\m{L}_{V}\m{T}_{:,k}
\end{equation}
with $len$ being the maximal length of the trajectory and $k$ the core
state selected. The probabilities to not reach any core is given by
\begin{equation}
  p_{not}=1-\sum_{k}C_{k}
\end{equation}


We can also get the probability to reach a state in exactly $n$ states
by
\begin{equation}
  p_{k}(n)=\m{R}_{V}\diag(\lambda_{V}^{n})\m{L}_{V}\m{T}_{:,k}
\end{equation}
with this object we can write the committor as
\begin{equation}
  C_{k}=\sum_{n=0}^{len}p_{k}(n)
\end{equation}



\subsection{Multi-state MFPT}

Average time to visit a state $i$ before visiting any other state
$j\neq i$.

\begin{equation}
  \m{M}=\text{cmfpt}\left[i\rightarrow j\mid\neg V\right]=\m{A}_{\v{x}\mid x_{0}=i,x_{i}\notin V}\left[\min_{n\ge1}x_{n}=j\right]
\end{equation}
Using OOM the expressions can be written down easily. I have not yet
found a way to reduce the complexity by one order, but it works. One
can also get the distribution of times since OOMs allow to specify
which type of trajectories should be used, e.g. we can ask what is the
average time to visit state $i$ among all trajectories that do not end
up prior in another core state $j$ and have a maximal length of
$t_{max}$.

From the complete distribution we can also compute higher moments like
variances of the \emph{mfpt} or \emph{confidence intervals}.  We
compute

\begin{eqnarray*}
  m_{k}(n) & = & C_{k}^{-1}\sum_{n=0}^{len}(1+k)p_{k}(n)\\
  & = & C_{k}^{-1}\m{R}_{V}\diag(\frac{1-(n+2)\lambda_{V}^{n+1}+(n+1)\lambda_{V}^{n+2}}{(1-\lambda_{V})^{2}})\m{L}_{V}\m{T}_{:,k}
\end{eqnarray*}
which simplifies to
\begin{eqnarray*}
  m_{k}(\infty) & = & C_{k}^{-1}\m{R}_{V}\diag((1-\lambda_{V})^{-2})\m{L}_{V}\m{T}_{:,k}
\end{eqnarray*}
and in the case, where we have only one core this should reduce to the
known mfpt as
\begin{eqnarray*}
  m_{k}(\infty) & = & \m{R}_{V}\diag((1-\lambda_{V})^{-2})\m{L}_{V}\m{T}_{:,k}
\end{eqnarray*}
which provides an alternative to the previously given expressions to
compute the mfpt. Similarly we can derive also variances in the mfpt
\begin{eqnarray*}
  v_{k}(n) & = & C_{k}^{-1}\sum_{n=0}^{len}(1+k)^{2}p_{k}(n)\\
  & = & C_{k}^{-1}\m{R}_{V}\diag(\frac{-\left(2n^{2}+6n+3\right)\lambda_{V}^{n+2}+(n+2)^{2}\lambda_{V}^{n+1}+(n+1)^{2}\lambda_{V}^{n+3}-\lambda_{V}-1}{(\lambda_{V}-1)^{3}})\m{L}_{V}\m{T}_{:,k}
\end{eqnarray*}
The most complex operation is the eigendecomposition which has to be
done once for a fixed set of core sets. The rest is only matrix
multiplications.


\section{Example}

{[}fill{]}


\section{Conclusion}

{[}fill{]}


\section{Acknowledgements}

Thank you's

\bibliographystyle{abbrvnat} \bibliography{complete}

\end{document}
