%% LyX 2.0.7.1 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[letterpaper,english,draft,aps,onecolumn,showkeys,nofootinbib,superscriptaddress,showpacs,floatfix]{revtex4}
\usepackage{mathpazo}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{esint}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
\pdfpageheight\paperheight
\pdfpagewidth\paperwidth


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\@ifundefined{textcolor}{}
{%
 \definecolor{BLACK}{gray}{0}
 \definecolor{WHITE}{gray}{1}
 \definecolor{RED}{rgb}{1,0,0}
 \definecolor{GREEN}{rgb}{0,1,0}
 \definecolor{BLUE}{rgb}{0,0,1}
 \definecolor{CYAN}{cmyk}{1,0,0,0}
 \definecolor{MAGENTA}{cmyk}{0,1,0,0}
 \definecolor{YELLOW}{cmyk}{0,0,1,0}
 }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage{mystyle}

\makeatother

\usepackage{babel}
\begin{document}

\title{Enhanced Markov State Model Generation using Multi-State Transition
Interface Sampling}


\author{Jan-Hendrik Prinz}


\affiliation{Memorial Sloan-Kettering Cancer Center, Zuckerman Research Institute,
Zuckerman Research Center, 417 East 68th Street, New York, NY 10065,
US}

\email{jan-hendrik.prinz@fu-berlin.de}


\author{Weina Du}

\email{weina.du@...}


\affiliation{University of Amsterdam, The Netherland}


\author{Peter Bolhuis}

\email{bolhuis@...}


\affiliation{University of Amsterdam, The Netherland}


\author{John D. Chodera}

\email{choderaj@mskcc.org}


\affiliation{Memorial Sloan-Kettering Cancer Center, Zuckerman Research Institute,
Zuckerman Research Center, 417 East 68th Street, New York, NY 10065,
US}
\begin{abstract}
The application of small-sized Markov state models (MSMs) has become
a standard tool for the analysis of the dynamics in molecular dynamics
(MD) simulations and computation of key properties. Here we present
a rigorous approach to enhance the generation of a MSM for a system
of biomolecules using multi-state transition interface sampling (MSTIS)
\end{abstract}

\pacs{{[}add{]}}


\keywords{Time Series Analysis, Timescale Estimation, Markov State Models,
Implied Timescales}

\maketitle

\section{Introduction}




\section{Considerations}

TIS uses interfaces

We want to find core sets and regions around it. For a single state
it would be best to use mean first passage time to 


\section{Distance Ideas from Transition matrices}


\subsection{Commute Distance}

The average roundtrip time between states $i$ and $j$

\[
\text{cd}\left[i\leftrightarrow j\right]=\tr{(\ve_{i}-\ve_{j})}\left(\mPi\left(\Id-\mT\right)\right)^{\text{-}}(\ve_{i}-\ve_{j})
\]



\subsection{Mean First Passage Time}

Definition of $\mA$ and $\mW$

\[
\mA\equiv\Id-\mT
\]


\begin{eqnarray*}
\mW & \equiv & \Id-\mA\mA^{\#}\\
 & = & \m{J}\mPi
\end{eqnarray*}
Generlized inverse (Use the Jordan Decomposition and use the pseudo
inverse only on the diagonal part

\begin{eqnarray*}
\mA^{\#} & \equiv & \text{JE}[\mA]\text{JD}\left[\mA\right]^{\dagger}\inv{\text{JE}[\mA]}\\
 & = & \inv{\left(\mA+\mW\right)}-\mW\\
 & = & \inv{\left(\Id-\mT+\m{J}\mPi\right)}-\m{J}\mPi
\end{eqnarray*}
The matrix $\mA^{\#}$ can be interpreted as a list of vectors that
contain the expected difference in counts between timeseries started
from equilibrium and timeseries started in each state.
\[
\mA_{j}^{\#}=\sum_{k=0}^{\infty}\left(\veT_{j}\mT^{k}-\vpiT\right)
\]
The indexing with \emph{dg, }like\emph{ }$\mA_{dg}$, means to take
matrix $\mA$ and set all off-diagonal entries to zero. Mean First
Passage Time

\begin{eqnarray*}
\mM=\text{mfpt}\left[i\rightarrow j\right] & = & \mathbb{E}_{\vx\mid x_{0}=i}\left[\min_{n\ge1}x_{n}=j\right]\\
 & = & \left(\Id-\mA^{\#}+\m{J}\mA_{dg}^{\#}\right)\mPiI\\
 & = & \left(\Id-\inv{\left(\mA+\mW\right)}+\mW+\m{J}\mA_{dg}^{\#}\right)\mPiI\\
 & = & \left(\Id-\inv{\left(\mA+\m{J}\mPi\right)}+\m{J}\mPi+\m{J}\mA_{dg}^{\#}\right)\mPiI
\end{eqnarray*}


This can be used to compute the expected time to reach a state when
started in equilibrium (first discovery time)

\begin{eqnarray*}
\text{fdt}[k] & = & \mathbb{E}_{\vx\mid x_{0}\sim\pi}\left[\min_{n}x_{n}=k\right]\\
 & = & \left(\vpiT\mM\right)_{k}\\
 & = & 1+\frac{\mA_{kk}^{\#}}{\pi_{k}}
\end{eqnarray*}


We can also compute the variance in the mean first passage time by

\begin{eqnarray*}
\mV_{ij} & = & \text{vmfpt}[i\rightarrow j]=\mathbb{E}_{\vx\mid x_{0}=i}\left[\left(\min_{n\ge1}x_{n}=j-\mM_{ij}\right)^{2}\right]\\
\mV_{ij} & = & \mB_{ij}-\left(\mM_{ij}\right)^{2}
\end{eqnarray*}
with the squared fluctuations given by

\[
\mB=\mM\left(2\mA_{dg}^{\#}\mD+\Id\right)+2\left(\mA^{\#}\mM-\m{J}(\mA^{\#}\mM)_{dg}\right)
\]
which can be simplified for the diagonal elements
\[
\mB_{dg}=2\mD\mA_{dg}^{\#}\mD+\mD
\]
I assume we could use this to check how fast a certain region might
be explored up to a certain percentage.


\subsection{Multi-state Committor}

Probability to enter a state $i$ before visiting any other state
$j\neq i$. 

Using OOM we can also change the expression for the committor to trajectories
of a maximal length which is very useful. See Multi-State MFPT for
the extension to the distribution for a trajectory length $k$.

This directly gives also the probability to not hit any core!

Assume that we have 
\[
\mT_{V}=\diag(\{\begin{cases}
1 & \text{if }i\in\mV\\
0 & \text{else}
\end{cases}\mid i\in\Omega\})\,\mT
\]
which is the the transition matrix with all columns set to zero that
belong to states that are not in $\mV$ the set of states not in a
core, also referred to as the \emph{void}.

We then split the rank deficient matrix into eigenvalues and left
and right eigenvectors like

\[
\mT_{V}=\mR_{V}\diag(\lambda_{V})\mL_{V}
\]
then the max length committor is given by
\[
C_{k}=\mR_{V}\diag(\frac{\lambda_{V}^{len}-1}{\lambda_{V}-1})\mL_{V}\mT_{:,k}
\]
with $len$ being the maximal length of the trajectory and $k$ the
core state selected. The probabilities to not reach any core is given
by
\[
p_{not}=1-\sum_{k}C_{k}
\]


We can also get the probability to reach a state in exactly $n$ states
by
\[
p_{k}(n)=\mR_{V}\diag(\lambda_{V}^{n})\mL_{V}\mT_{:,k}
\]
with this object we can write the committor as
\[
C_{k}=\sum_{n=0}^{len}p_{k}(n)
\]



\subsection{Multi-state MFPT}

Average time to visit a state $i$ before visiting any other state
$j\neq i$.

\[
\mM=\text{cmfpt}\left[i\rightarrow j\mid\neg V\right]=\mathbb{E}_{\vx\mid x_{0}=i,x_{i}\notin V}\left[\min_{n\ge1}x_{n}=j\right]
\]
Using OOM the expressions can be written down easily. I have not yet
found a way to reduce the complexity by one order, but it works. One
can also get the distribution of times since OOMs allow to specify
which type of trajectories should be used, e.g. We can ask what is
the average tome to visit state $i$ among all trajectories that do
not end up prior in another core state $j$ and have a maximal length
of $t_{max}$. 

From the complete distribution we can also compute higher moments
like variances of the mfpt or confidence intervals. We compute

\begin{eqnarray*}
m_{k}(n) & = & C_{k}^{-1}\sum_{n=0}^{len}(1+k)p_{k}(n)\\
 & = & C_{k}^{-1}\mR_{V}\diag(\frac{1-(n+2)\lambda_{V}^{n+1}+(n+1)\lambda_{V}^{n+2}}{(1-\lambda_{V})^{2}})\mL_{V}\mT_{:,k}
\end{eqnarray*}
which simplifies to
\begin{eqnarray*}
m_{k}(\infty) & = & C_{k}^{-1}\mR_{V}\diag((1-\lambda_{V})^{-2})\mL_{V}\mT_{:,k}
\end{eqnarray*}
and in the case, where we have only one core this should reduce to
the known mfpt as
\begin{eqnarray*}
m_{k}(\infty) & = & \mR_{V}\diag((1-\lambda_{V})^{-2})\mL_{V}\mT_{:,k}
\end{eqnarray*}
which provides an alternative to the previously given expressions
to compute the mfpt. Similarly we can derive also variances in the
mfpt
\begin{eqnarray*}
v_{k}(n) & = & C_{k}^{-1}\sum_{n=0}^{len}(1+k)^{2}p_{k}(n)\\
 & = & C_{k}^{-1}\mR_{V}\diag(\frac{-\left(2n^{2}+6n+3\right)\lambda_{V}^{n+2}+(n+2)^{2}\lambda_{V}^{n+1}+(n+1)^{2}\lambda_{V}^{n+3}-\lambda_{V}-1}{(\lambda_{V}-1)^{3}})\mL_{V}\mT_{:,k}
\end{eqnarray*}
The most complex operation is the eigendecomposition which has to
be done once for a fixed set of core sets. The rest is only matrix
multiplications.


\section{Theory}

Let $\{x_{t}\mid x_{t}\in\Omega,t\in\mathbb{R}^{+}\}$ be the (stochastic)
process of interest in full state space $\Omega$ which we require
to be Markovian, time-homogeneous, ergodic and reversible wrt. its
unique stationary distribution $\mu$. For a time-homogeneous Markov
process the time evolution of the probability density $p_{t}(x)\equiv\mathbb{P}(x_{t}=x)$
can be expressed using a time-step $\tau$ (also called lag time)
dependent propagator $\mathcal{P}_{\tau}$ by \cite{Schutte:1999ju,Schutte:2003ty}
\begin{eqnarray*}
p_{t}(x) & = & \mathcal{P}_{\tau}\left[p_{t}\right](x)\\
 & \equiv & \int_{\Omega}\text{d}x'\,\mathbb{P}\left[x_{t+\tau}=x\mid x_{t}=x'\right]p_{t}(x').
\end{eqnarray*}
Ergodicity and reversibility of $\{x_{t}\}$ then implies that $\mathcal{P}_{\tau}$
is compact and self-adjoint wrt. the stationary density $\mu$ on
a Hilbert space defined by the inner product
\[
\langle u,v\rangle\equiv\int_{\Omega}\text{d}x\, u(x)v(x)\mu^{-1}(x)
\]
and we can expand the propagator into a countable sum 
\[
\mathcal{P}_{\tau}p_{t}=\sum_{k=1}^{\infty}\lambda_{i}(\tau)\phi_{i}\langle\phi_{i},p_{t}\rangle
\]
of orthogonal eigenfunction/eigenvalue pairs $\{(\lambda_{i}(\tau),\phi_{i})\mid i\in\mathbb{N}\}$
with $\langle\phi_{i},\phi_{i}\rangle=\delta_{ij}$ and $\Vert\lambda_{i}\Vert\leq1$.
For convenience we order all pairs with descending magnitude of the
eigenvalue where the uniqueness of the stationary distribution ensures
that the eigenvalue of one $\lambda_{1}$ is single and so $\phi_{1}=\mu$.
The semi-group property of a markov chain also implies a relation
$\lambda_{i}(k\tau)=\lambda_{i}^{k}(\tau)$ s.t. we can define $\lambda_{i}^{\tau}\equiv\lambda_{i}^{\tau}(1)$
as the eigenvalues at the native lag time $\tau=1$. 

To describe metastability which we required we use that metastability
leads to gaps in the spectrum of the propagator \cite{bovier:2005a}
where the finite set of $m$ eigenvalue/eigenfunction pairs above
the gap (being slower) describe the metastable dynamics while the
remaining faster pairs describe the fast dynamics within metastable
sets. That these separation is possible and the dynamics do not mix
follows from the orthogonality of the eigenfunctions. Thus we can
split the propagator
\begin{eqnarray}
\mathcal{P}_{\tau} & = & \mathcal{P}_{\tau}^{\text{meta}}+\mathcal{P}_{\tau}^{\text{fast}}\nonumber \\
 &  & \sum_{k=1}^{m}\lambda_{i}(\tau)\phi_{i}\langle\phi_{i},\,\cdot\,\rangle+\mathcal{P}_{\tau}^{\text{fast}}\label{eq:metastable-dynamics}
\end{eqnarray}
with $\Vert\mathcal{P}_{\tau}^{\text{fast}}\Vert\in\mathcal{O}\left(\lambda_{m+1}^{\tau}\right)$
and conclusively, if $\tau\gg-1/\log\lambda_{m+1}$ the fast part
of $\mathcal{P}_{\tau}$ has decayed and the dynamics can be well
approximated by the metastable part $\mathcal{P}_{\tau}\approx\mathcal{P}_{\tau}^{\text{meta}}$.
In this paper we will be dealing with this type of dynamics and call
this an ``$m$-metastable'' system. 

Usually, the full dynamics is not directly observable, either by an
artificially induced discretization, the experimental setup or unavoidable
sideeffects like noise. Instead we have to deal with a projected process
$\{y_{t}\mid y_{t}\in\Upsilon,t\in\mathbb{R}^{+}\}$ on an observable
space $\Upsilon=\{1,\ldots,K\}$ with $K$ states which we choose
to be finite. To describe the probabilistic relation between $x_{t}$
and $y_{t}$ we use a output probability functions $\chi_{i}$ so
that
\[
\chi_{k}(x)\equiv\mathbb{P}\left(y_{t}=k\mid x_{t}=x\right)
\]
which explicitely assumes that the output probabilities are time independent
and also contain no memory s.t. each state $x\in\Omega$ in the full
state space always maps into $\Upsilon$ in the same way. The combination
of the reversible Markov state model and output probabilities will
be called a \emph{projected Markov model }(PMM), if in addition the
full state dynamics is $m$-metastable we will refer to $m$-metastable
PMM or short $m$-PMM. Note, that $\{y_{t}\}$ is \emph{not} a Markov
chain anymore unless the output probability functions exactly span
the space of the $m$ dominant eigenfunctions, which is either impossible
due to experimental constraints or requires explicit knowledge of
the propagator. Therefore the popular modelling using a Markov state
model (MSM) is often only approximate \cite{Sarich:2010wa}. 

To capture the dynamics of a PMM we compute the state-to-state correlation
matrix $C_{ij}(\tau)$ in the observable space 
\begin{eqnarray*}
C_{ij}(\tau) & \equiv & \mathbb{P}\left[y_{t+\tau}=j\,\wedge\, y_{t}=i\right]\\
 & = & \iint_{\Omega^{2}}\text{d}x\,\text{d}x'\,\chi_{i}(x)\mathbb{P}\left[x_{t+\tau}=x\,\wedge\, x_{t}=x'\right]\chi_{j}(x')
\end{eqnarray*}
for the stochstic process $y_{t}$ which implicitely assumes infinite
statistics. In this case the expression can be rewritten as
\begin{eqnarray*}
C_{ij}(\tau) & = & \sum_{k=1}^{\infty}\lambda_{i}(\tau)\int_{\Omega}\text{d}x\,\chi_{i}(x)\phi_{k}(x)\int_{\Omega}\text{d}x'\,\phi_{k}(x')\chi_{j}(x')\\
 & = & \sum_{k=1}^{\infty}\lambda_{i}(\tau)Q_{ki}\, Q_{kj}
\end{eqnarray*}
with the matrix $Q$ of \emph{projected eigenfunctions }
\[
Q_{ki}\equiv\int_{\Omega}\text{d}x\,\chi_{i}(x)\phi_{k}(x).
\]
Using the diagonal matrix of eigenvalues $\Lambda\equiv\text{diag}\left(\lambda_{1},\ldots\right)$
we can finally write
\[
C(\tau)=Q^{\text{T}}\Lambda(\tau)Q
\]
which is exact for any PMM but might require countable, but infinite
sized $Q$ and $\Lambda$. For a $m$-PMM the metastability reduces
this size to $Q\in\mathbb{R}^{m\times K}$ and $\Lambda\in\mathbb{R}^{m\times m}$
which is much more tractable. We conclude that all correlation functions
on the observable space of a $m$-PMM can exactly be represented by
the parametrization into $Q$ and $\Lambda$.


\section{Spectral Estimation}

The goal will be to construct a learning algorithm to estimate $Q$
and $\Lambda$ from a given observation $\{y_{t}\}$. This will allow
us to correctly estimate all dominant eigenvalues $\lambda_{i}$ of
the full propagator. As input for the learning algorithm we use the
state-to-state correlation matrices $C(\tau)$, which can easily be
estimated from a finite timeseries. Since $C(\tau)$ is by construction
symmetric %
\footnote{The real $C(\tau)$ is symmetric for reversible dynamics, while an
estimated $C(\tau)$ might not, depending on the estimation procedure.
Also if the the used data is finite and is not sampled from global
equilibrium the estimated $C(\tau)$ is not symmetric!%
}, the number of independent variables in a single $C(\tau)$ is smaller
than the variables to be parametrized in $Q$ and $\Lambda(\tau)$.
Conclusively, a single $C(\tau)$ does not contain enough information
to allow for a unique decomposition of a given $C(\tau)$ into $Q$
and $\Lambda$. In MSMs this lack of information is filled by an orthogonality
constraint 
\[
Q^{T}Q=\text{diag}(\pi)
\]
wrt. the observed stationary distribution $\pi$. Other decompositions
of this type include the singular value decomposition for symmetric
matrices (SVD) or, equivalently, the eigenvalue decomposition (EVD)
which both assume orthogonality $Q^{T}Q=\text{Id}$ or the Cholesky
decomposition that instead uses a triangular shaped $Q$. It can be
shown (see SI), that two correlation matrices at two different lagtimes
$\tau_{1}$ and $\tau_{2}$ provide enough information to solve this
problem uniquely in the case of an $m$-PMM and positive eigenvalues.

We will first address the special case where the number of dominant
eigenvalues~$m$ equals the number of observable states $K$ so that
$Q$ is square. This case can always be achieved by an additional
projection onto $m$ states provided that initially the number of
observed states is larger than the number of processes $K>m$. The
problem then (see SI) reduces to a solving the generalized hermitian
eigenvalue problem (GHEP)
\[
C(\tau_{1})u_{i}=\nu_{i}C(\tau_{2})u_{i}
\]
for the generalized eigenvalues $\nu$ and eigenvectors $u_{i}$ that
are related to the dominant eigenvalues by
\[
\lambda_{i}=\nu_{i}^{1/(\tau_{1}-\tau_{2})}
\]
to compute the matrix $Q$ of projected eigenvectors by
\[
Q=U^{-1}
\]
where $U=[u_{1},\ldots,u_{m}]$ is the matrix of the generalized eigenvectors
$u_{i}$ that are normalized so that 
\[
\Vert u_{i}\Vert^{2}=\lambda_{i}^{\tau_{1}}\left(u_{i}^{T}C(\tau_{1})u_{i}\right)^{-1}
\]
holds. Note, that a GHEP can also be solved using an inversion of
one matrix, but this is not necessary. Also this transforms into a
non-symmetric problem and the inversion might be badly conditioned.
Nonetheless, we can compute the solutions using an ordinary EVD of
$C^{-1}(\tau_{2})C(\tau_{2})$. 


\section{Low-rank approximation}

In most cases we are only interested in the dominant processes that
govern the dynamics as stated in Eq.~\ref{eq:metastable-dynamics}.
Since the spectral estimation method is (almost) independent of the
projection we can project onto a set of $N$ states, then apply the
spectral estimation method and retrieve the dominant timescales. If
we also want to reconstruct the dominant processes on the complete
observable space we need to use a generating set of vectors for the
space spanned by the projected left eigenvectors. Since we have no
direct access to the eigenvectors we can use the singular value decomposition
\begin{equation}
C(\tau)\rightarrow V(\tau)\Sigma(\tau)V^{T}(\tau)\label{eq:svd-decomposition}
\end{equation}
which for a symmetric matrix provides for a decomposition into a matrix
$V$ and a diagonal matrix $\Sigma$. If for a particular $\tau>\tau_{\text{min}}$
only $m$ singular values exist then $V$ must span the same subspace
of $\Upsilon$ as $Q$ which follows from rank considerations. As
stated before, the decomposition in Eq.~\ref{eq:svd-decomposition}
is not unique and the SVD requires $V^{T}V=\text{Id}$ which is not
the correct solution. The most general form the correct solution can
take is $Q=AV$ with $A$ being an $m\times m$ matrix that captures
the linear recombination and a second correlation matrix $C(\tau_{2})$
makes the decomposition unique. Finally, we propose the following
procedure to compute the dominant $Q$ and $\Lambda$
\begin{enumerate}
\item Estimate a reasonable range of lagtimes using the singular value decomposition
(SVD) which hints to the number $N$ of processes needed to approximate
the dynamics,
\item Select two lagtimes $\tau_{1}$ and $\tau_{2}$ which lie in the range
that at least $m$ singular values are non-zero.
\item Compute the two symmetric correlation matrices $C(\tau_{1})$ and
$C(\tau_{2})$
\item Compute the low-rank approximation of rank $m$ using the SVD of $C(\tau_{1})=V\Sigma V^{T}$
with $V\in\mathbb{R}^{K\times m}$ and $\Lambda\in\mathbb{R}^{m\times m}$ 
\item Compute the projections 
\[
B(\tau)\equiv V^{T}C(\tau_{1})V
\]
onto the sub-space spanned by the dominant $m$ singular vectors for
both lagtimes $B_{1}\equiv B(\tau_{1})=\Sigma,\, B_{2}\equiv B(\tau_{2})$
\item Compute the spectral decomposition (SD) by solving the GHEP $B_{1}u_{i}=\nu B_{2}u_{i}$
as given above to get $A$ and $\Lambda$ so that we can write $B(\tau)=A^{T}\Lambda(\tau)A$.
\item The final approximation is given by $Q=V\, A^{T}$ and $\Lambda$.
\end{enumerate}

\section{Example}

We demonstrate the improved estimation qualities of the new method
at a simple 2D-model that represents a case where MSMs fail. Fig.~\ref{fig:Energy-Landscape}
shows the stationary distribution of the overdamped langevin process
in a potential $U$ at inverse temperature $\beta=1$. The system
is designed to consist of 4 distinct metastable states labeled $A$
to $D$ in Fig.~\ref{fig:Energy-Landscape}. The process $x_{t}\in\Omega$
in the full state space is also projected onto the second ($y$) coordinate
which is then clustered onto 28 equidistant bins ($\Delta x=0.05$)
as $y_{t}$ 
\[
x_{t}\in\Omega\mapsto x_{t}^{(2)}\mapsto y_{t}\in\Upsilon=\{1,\ldots,28\}
\]
where the bins comprise the observable space $\Upsilon=\{1,\ldots,28\}$.
For the computation of reference values we also discretized the system
into a set of $28\times28$ equidistant bins while the SE and MSM
is applied to the observed space only. Two cases of data are considered
(I) infinite statistics $L\rightarrow\infty$ and (II) a timeseries
of length $L=10^{6}$ which is long compared to the slowest relaxation
timescale of $t_{2}=382.4$ and can thus be considered a case of converged
and good, but finite statistics.

Fig.~\ref{fig:Singular-values} shows the singular values of $C(t)$
computed from the timeseries and show the presence of the 4 distinct
timescales as anticipated from the shape of the stationary distribution.
The plot allows to estimate that for lagtimes $\tau>125$ the low-rank
approximation of the 4 dominant processes (eigenvectors) is best.

Fig.~\ref{fig:Estimation-results} presents the estimation results
(see caption for details) indicating the the SE estimator allows for
a much faster convergence of the estimated timescales to the true
values in comparison to the MSM approach. It is also able to much
more accurately predict the non-orthogonal projected eigenvectors
$Q$. 




\section{Multi-Lagtime Estimation}

To address the issue of statistical uncertainties we rewrite the problem
into an optimization problem. We use the list of time scales used
in the estimation $\mathcal{T}=\{\tau_{1},\tau_{2}\}$ and write

\[
\left\{ Q,\Lambda\right\} =\underset{Q,\Lambda}{\text{argmin}}\sum_{\tau\in\mathcal{T}}\Vert C(\tau)-Q^{\text{T}}\Lambda(\tau)Q\Vert_{\text{F}}
\]
which is equivalent of estimation the parameters of a PMM as described
above. This can be generalized to more than 2 lagtimes by expanding
$\mathcal{T}$. Fig.~\ref{fig:StatisticalUncertainty} shows estimation
results from the multi-$\tau$ estimation for the 2D example from
Fig.~\ref{fig:Energy-Landscape}. For this we simulated 100 trajectories
starting in the center of state $B$ for 6 different lagtime lengths
$L\in\{50k,80k,100k,150k,200k,500k\}$. With increasing length $L$
the distribution becomes more narrow providing a more reliable estimate.
The MSM estimate (lighter colors) show a systematically too small
estimate compared to the spectral estimations (darker colors).


\section{Apo-Myoglobin Example}

We demonstrate the method on the Apo-Myoglobin. {[}Copy experimental
details from PRX publication{]}.

In order to illustrate the performance of spectral estimation on real
data, it is applied to optical tweezer measurements of the extension
fluctuations of a biomolecule examined in a recent optical force spectroscopy
study: the H36Q mutant of sperm whale apo-myoglobin at low pH \cite{Elms:2012jb}.
The apo-myoglobin {[}crystal structure shown in Fig. 3(4){]} hops
between unfolded and molten globule states at the experimental pH.
Experimental force trajectory data were generously provided by the
authors of Refs. \cite{Elms:2012jb}. Experimental details are given
therein, but we briefly summarize aspects of the apparatus and experimental
data collection procedure relevant to our analysis. The instrument
used to collect the data sets was a dual-beam counterpropagating optical
trap \cite{Bustamante:wb}. The molecule of interest was tethered
to polystyrene beads by means of dsDNA handles, with one bead suctioned
onto a pipette and the other held in the optical trap. A piezoactuator
controlled the position of the trap and allowed position resolution
to within 0.5 nm, with the instrument operated in passive (equilibrium)
mode such that the trap was stationary relative to the pipette during
data collection. The force on the bead held in the optical trap was
recorded at 50 kHz, with each recorded force trajectory 60 s in duration
resulting in $3\cdot10^{6}$ data points each. From the set of presentes
fibers we chose to analyze fiber no. 6. {[}check how this is mentioned
in Ref. \cite{Elms:2012jb}{]}

Fig.~\ref{fig:ApoMyoglobin} shows estimation results for $\mathcal{T}=\{150,\ldots,500\}$
and computing the projective SVD also at $\tau=150$. The estimations
show a consistent estimation of two processes {[}maybe we can find
a fourth one for 15-17? Actually I checked and it looks like it, but
haven't finished this yet.{]} For \#IDs 15-17 the blue process seems
to switch to a different one at lower bead extensions. 

For further analysis we constructed a \emph{kinetic map }{[}cite GuilleEtAl,
tICA and kinetic maps{]} using that visualizes the shift in the dominant
slow processes with increasing force in the trap settings. {[}I have
also finished a PCCA+ based MSM using the spectral estimation. This
leads to transition models, see Fig.~ which are consisten with what
we see in the kinetic map. I can also compute a nice plot of stationary
distributions. See{]}






\section{Conclusion}

We presented a novel approach to estimate the parameters $Q$ and
$\Lambda$ of a projected Markov state model (PMM). The method has
been demonstated at a simple illustrative model and shows much improved
convergence in the estimation of the dominant timescales as well as
related projected eigenvectors. Although the sensitivity to statistical
noise has increased the method still provides better results compared
to the MSM approach. The sensitivity can be reduced if the estimation
is reformulated into an optimization problem that fits the result
at more than two lagtimes. We have applied the optimizational approach
to data from Apo-Myoglobin and showed a consistent set of two processes
(maybe four). On first sight the estimation for $m$-PMMs looks identical
to and $m$-state HMM, but it can be shown that HMMs are not able
to capture all possible dynamics of $m$-PMMs and HMMs can model a
subset of the dynamics of PMMs. Thus the HMM solution might be close,
but rarely exact\cite{Noe:2013tx}. Finally, the cases of low-statistics
and non-reversible dynamics is interesting and subject of active research.


\section{Acknowledgements}

The authors would like to thank Ralf Banisch for stimulating discussions
on the topic. JHP acknowledges support from the DFG research center
\textsc{Matheon}. HW {[}missing{]} . FN acknowledges funding from
DFG grant NO 825/2-2 and ERC starting grant 'pcCell' \textquotedbl{}.
JDC acknowledges funding from {[}@JDC: missing{]}

\bibliographystyle{apsrmp}
\bibliography{complete}

\end{document}
